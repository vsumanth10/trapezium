runMode = "STREAM"
dataSource = "HDFS"
dependentWorkflow = "batchWorkFlow"
#HDFS Stream
#if your application is streaming data from HDFS, provide streaming details
#checkpointDirectory - this directory will be used to break RDD lineage
#checkpointInterval - time in ms for checkpointing stream
#batchTime - duration of each batch
#streamsInfo - It is a list of streams that you want Application Manager to create
#streamsInfo.name - name of hdfs stream. Workflow components will provide this name if they are interested in this stream
#streamsInfo.dataDirectory - HDFS directory to create Spark DStream
hdfsStream = {
  checkpointDirectory = "checkpoint"
  checkpointInterval = "3000000"
  batchTime = "5"
  streamsInfo = [{
    name = "hdfsStream"
    validation = {
      columns = ["router","interface","oid","eventdate","elapsetime","inbytes","outbytes"]
      datatypes = ["String","String","Integer","Date","Long","Double","Double"]
      dateFormat = "yyyy-MM-dd HH:mm:ss"
      delimiter = "|"
      minimumColumn = 7
      rules = {}
    }
    dataDirectory = {
      local ="/user/plet/data"
      dev = "/midm/"
      prod = ""
    }
  }]
}
transactions = [{
  transactionName = "com.verizon.bda.trapezium.framework.apps.AppETL"
  inputStreams = [{
    name: "hdfsStream"
  }]
  persistStreamName = "hdfsFiltered"
  isPersist = "false"
}, {
  transactionName = "com.verizon.bda.trapezium.framework.apps.AlgorithmETL"
  inputStreams = [{
    name: "hdfsFiltered"
  }]
  persistStreamName = "hdfsDerived"
  isPersist = "false"
}, {
  transactionName = "com.verizon.bda.trapezium.framework.apps.AlgorithmEval"
  inputStreams = [{
    name: "hdfsDerived"
  }]
  persistStreamName = "hdfsAnomalies"
  isPersist = "true"
}]
